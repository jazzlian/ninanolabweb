<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Unknown</title>
  <link href="../Styles/stylesheet.css" rel="stylesheet" type="text/css" />
  <link href="../Styles/page_styles.css" rel="stylesheet" type="text/css" />
</head>

<body class="calibre">
  <h1 class="tochead">3 Graph Embeddings</h1>

  <p class="colisthead">This chapter covers</p>

  <p class="colistbulletcxspfirst">·&nbsp;&nbsp;&nbsp;&nbsp;Understanding graph embeddings and their limitations</p>

  <p class="colistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp;Using transductive and inductive techniques to create node embeddings</p>

  <p class="colistbulletcxsplast">·&nbsp;&nbsp;&nbsp;&nbsp;Taking the example dataset from chapter 3 to creating node embeddings</p>

  <p class="body">Graph embeddings are low-dimensional representations of graphs, and can be generated for entire graphs, sub graphs, nodes, and edges.&nbsp; Graph embeddings can be generated in several ways, including with graph algorithms, linear algebra methods, and GNNs.</p>

  <p class="body">GNNs are special because embedding is inherent to their architectures. Previously when there was a need for a graph embedding in a machine learning application, the embedding and the model training were done with separate processes. With GNNs, embedding and model training are done with one algorithm.</p>

  <p class="body">This chapter covers graph representations, particularly node embeddings. We’ll examine this space in general, and look at two ways of creating graph embeddings: using transductive and inductive methods. As part of this process, we’ll create our first output from a GNN.</p>

  <p class="body">Throughout this chapter, we’ll use the social network data discussed previously.</p>

  <h4 class="head2 sigil_not_in_toc">The problem: Node Embeddings on a Social Graph</h4>

  <p class="body">In the previous chapter, we outlined the creation of a social graph created by a recruiting firm. Nodes are job candidates, and edges represent relationships between job candidates. We generated graph data from raw data, in the form of edge lists and adjacency lists. We then used that data in a graph processing framework (NetworkX) and a GNN library (Pytorch Geometric). The nodes in this data included the candidate’s <i class="charitalics">ID</i>, <i class="charitalics">job type</i> (accountant, engineer, etc), and <i class="charitalics">industry</i> (banking, retail, tech, etc).</p>

  <p class="body">The objective for this chapter is to generate node embeddings from our graph. As shown in figure 3.1, we’ll start with an adjacency list, do needed preprocessing, then use two techniques to create and visualize our node embeddings.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_01.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.1 . A high-level process for this chapter.</p>
  </div>

  <p class="body">Onward, the next chapters will focus on the entire process of training GNN models, which in no small part rely on their embedding mechanisms.</p>

  <p class="body">For this chapter, first we’ll do a deeper dive into embeddings as representations of graph entities. We’ll examine and try transductive and inductive methods. In section 3.2, we’ll use Node2Vec, a transductive technique, then in section 3.3, we’ll use a graph neural network to generate the embeddings.</p>

  <h2 class="head" id="sigil_toc_id_32">3.1 Graph Representations II</h2>

  <p class="body">In sections 1.1.1 and 2.2, we touched on the notion of data representations. We said that a core sub task of ML is to find ways to present data to our algorithms that allow them to learn from it. ML algorithms also output representations for use in downstream tasks. In this section we’ll explore this concept more deeply and highlight its relevance to GNNs.</p>

  <h3 class="head1" id="sigil_toc_id_33">3.1.1 Overview of Embeddings</h3>

  <p class="body">A <b class="charbold">Data Representation</b> is a way of displaying, formatting, or showing data for some usage.</p>

  <p class="body">In language, a word can be written in different languages, different fonts. That work can be written, spoken, embossed in Braille, or tapped out in Morse code.</p>

  <p class="body">Numbers can be written in Arabic, Roman, or Chinese characters. These numbers can be expressed in Binary, Hex, or Decimal notations. They can be written, spoken, or we can use our feet to count the number by stomping the ground. A given set of digital information can be shown or presented to software in many ways.</p>

  <p class="body">The usefulness of a representation is tied to the particular task of relevance. For our language example, effectively communicating a message requires a particular representation of words. If I want to order a pizza, writing out the order on paper won’t help, even if all the details of the order are captured on paper. I could mail or fax the written order; but the fastest way is probably to speak the order over the phone.</p>

  <p class="body">A numeric example (from the <i class="charitalics">Deep Learning</i> textbook) is doing long division given two numbers. If the numbers are presented to the problem solver in any other way than arabic numerals, the solver will convert the numbers to the Arabic representation to solve the problem. So, whether the numbers are spoken, written out in Roman Numerals, or tapped out in morse code, before solving the division task the solver will convert the numbers to the arabic representation.</p>

  <p class="body">Bringing this back to graphs and machine learning, in chapter 2, we covered a few ways to represent a graph, including using adjacency matrices and edge lists. We also discussed high-level ways that graphs are represented to software, such as using arrays and hashes.</p>

  <p class="body">Adjacency matrices and edge lists representations are powerful enough to perform many graph analytical methods.&nbsp; Such data models enable network traversal on which many analytical methods are based. However, an adjacency matrix cannot convey more rich information about a network, including the features and properties of its elements, and more subtle topological information.</p>

  <table cellpadding="0" cellspacing="0" class="msonormaltable1" width="100%">
    <tr class="calibre10">
      <td char="33%" class="calibre23">
        <p class="tablehead">Representation</p>
      </td>

      <td char="39%" class="calibre24">
        <p class="tablehead">Description</p>
      </td>

      <td char="26%" class="calibre25">
        <p class="tablehead">Examples</p>
      </td>
    </tr>

    <tr class="calibre10">
      <td char="33%" class="calibre26">
        <p class="tablebody">Basic Data Representations</p>
      </td>

      <td char="39%" class="calibre27">
        <p class="tablelistbulletcxspfirst">·&nbsp;&nbsp;&nbsp;&nbsp; Great for analytical methods that involve network traversal</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Useful for some node classification algorithms</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Information provided: Node and edge neighbors</p>
      </td>

      <td char="26%" class="calibre28">
        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Adjacency list</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Edge list</p>

        <p class="tablelistbulletcxsplast">·&nbsp;&nbsp;&nbsp;&nbsp; Adjacency matrix</p>
      </td>
    </tr>

    <tr class="calibre10">
      <td char="33%" class="calibre26">
        <p class="tablebody">Transductive (shallow) Embeddings</p>
      </td>

      <td char="39%" class="calibre27">
        <p class="tablelistbulletcxspfirst">·&nbsp;&nbsp;&nbsp;&nbsp; Useless for data not trained on Difficult to scale</p>
      </td>

      <td char="26%" class="calibre28">
        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Deepwalk</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Node2Vec</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; TransE</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; RESCAL</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Graph Factorization</p>

        <p class="tablelistbulletcxsplast">·&nbsp;&nbsp;&nbsp;&nbsp; Spectral Techniques</p>
      </td>
    </tr>

    <tr class="calibre10">
      <td char="33%" class="calibre26">
        <p class="tablebody">Inductive Embeddings</p>
      </td>

      <td char="39%" class="calibre27">
        <p class="tablelistbulletcxspfirst">·&nbsp;&nbsp;&nbsp;&nbsp; Models can be generalized to new and structurally different graphs</p>

        <p class="tablelistbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;&nbsp; Represent</p>
      </td>

      <td char="26%" class="calibre28">
        <p class="tablelistbulletcxsplast">·&nbsp;&nbsp;&nbsp;&nbsp; GNNs can be used to inductively generate embeddings</p>
      </td>
    </tr>
  </table>

  <p class="body">To glean more information about a graph and its components, we turn to embedding techniques. An <b class="charbold">embedding</b> is a numerical representation of a graph, node, or edge that conveys information that can be used in multiple contexts.</p>

  <p class="body">For a graph, a node, or an edge, this numerical representation can be expressed in the form of a vector <b class="charbold">x</b> that has <b class="charbold">d</b> number of dimensions, or <span class="chartimesitalic"><span class="calibre29">x</span></span> <span class="chartimes"><span class="calibre30">∈</span></span> <span class="chartimes"><span class="calibre29">R</span></span><span class="charsuperscriptitalic">d</span>. This <b class="charbold">vector</b> representation is called a <b class="charbold">low-dimensional</b> representation of the entity.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_02.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.2. A graph (left), and its two-dimensional representation.</p>
  </div>

  <p class="body">When reduced to two or three dimensions, vector representations allow us to create visualizations that can be used to inspect a graph. For example, in the figure above, we observe that nodes that are far apart in the graph, are far apart in the 2-D scatter plot.</p>

  <p class="body">The way an embedding is created determines the scope of its subsequent usage. Here we examine embedding methods that can be broadly classified as <b class="charbold">transductive</b> and <b class="charbold">inductive</b>.</p>

  <p class="body">Both methods have their trade-offs, and either can shine given the right problem.</p>

  <p class="body"><b class="charbold">Inductive</b> learning techniques are equivalent to supervised learning. These learning methods refine a model using training data, verifies the model performance using test data, and is applied to newly observed data outside the training and test sets. The aim is to create a model which generalizes its training. As long as the new data follows the same distributional assumptions of the training and test data, there should be no need to retrain an inductive model.</p>

  <p class="body"><b class="charbold">Transductive</b> learning techniques contrast with inductive learning in a few key ways:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;<b class="charbold">Closed set of data for both training and prediction</b>. Transductive algorithms are not applied to new data outside of that used to train them.</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;<b class="charbold">Labeled and Unlabeled data are used in training</b>. Transductive algorithms make use of characteristics of unlabeled data such as similarities or geometric attributes to distinguish them.</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;<b class="charbold">The training directly outputs our predictions.</b> Unlike inductive models, there is no predictive model, only a set of predictions for our data points.</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;<b class="charbold">If we want to predict on new data, we must retrain our model.</b> Basically restating the first point.</p>

  <p class="body">What are the advantages of transductive models? The major advantage is that we can reduce the scope of the prediction problem. For induction, we are trying to create a generalized model that can be applied to any unseen data. For transduction, we are only concerned with the data we are presented with.</p>

  <p class="body">Disadvantages of transductive learning are that for many data points, this can be computationally costly. Also, our predictions can’t be applied to new data; in that case the model must be retrained.</p>

  <p class="body">In the context of node embedding, for a given graph, a transductive technique will directly produce embeddings. Thus, the result is essentially a lookup table. To make an embedding for a newly introduced node, it will be necessary to retrain using the complete set of nodes, including the new node.</p>

  <div class="calibre15">
    <p class="sidebarhead">Inset: Summary of Terms Related to Transductive Embedding Methods</p>
  </div>

  <p class="sidebarbody">Two additional terms related to transductive embedding methods and sometimes used interchangeably with it are <b class="charbold">shallow embedding methods</b>, and <b class="charbold">encoders</b>. Here, we will briefly distinguish these terms.</p>

  <p class="sidebarbody">&nbsp;</p>

  <p class="sidebarbody"><i class="charitalics">Transductive</i> methods, explained above, are a large class of methods of which graph embedding is one application. So, outside of our present context of representation learnings, the attributes of transductive learning remain the same.</p>

  <p class="sidebarbody">&nbsp;</p>

  <p class="sidebarbody">In machine learning, <i class="charitalics">shallow</i> is often used to refer to non-deep learning models or algorithms. Such models are distinguished from deep learning models in that they don’t use multiple processing layers to produce an output from input data. In our context of graph/node embeddings, this term also refers to methods that are not deep learning based, but more specifically points to methods that mimic a simple lookup table, rather than a generalized model produced from a supervised learning algorithm.</p>

  <p class="sidebarbody">&nbsp;</p>

  <p class="sidebarbody">Given the point of view that a shallow embedding is a lookup table, the ‘model’ that the transductive (or shallow) method produces is called an <i class="charitalics">encoder</i>. This encoder simply matches a given node (or graph) to its respective embedding in low dimensional space.</p>

  <div class="calibre16">
    <p class="sidebarend">&nbsp;</p>
  </div>

  <h2 class="head" id="sigil_toc_id_34">3.2 Transductive Embedding Technique: Node2Vec</h2>

  <p class="body">Now we turn our attention to transductive techniques applied to graph embeddings. The goal of such methods is to deliver embeddings at the graph- or node-level that reflect the similarities, relationships, and structure of the given graphs. Since these are transductive methods, the resulting embeddings only apply to the given dataset, meaning they aren’t valid for unseen nodes or graphs.</p>

  <p class="body">We’ll focus on node embeddings. Random Walk methods are one type of transductive method for node embeddings. Of these, two well known methods are DeepWalk and Node2Vec. Such embedding methods borrow heavily from concepts in word embedding from NLP. Such methods embody a few concepts:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;Establishing <b class="charbold">Node-to-Node Similarity</b>, or node context by performing <b class="charbold">random walks</b>.</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;<b class="charbold">Optimizing using these similarities</b> to get embeddings that can predict of the members of a node’s neighborhood</p>

  <p class="body">In the context of node embeddings, the characteristics and tradeoffs discussed here and in the previous section apply. We must also note an additional limitation: node embedding methods don’t take into account node attributes or features. So, if there is rich data associated with our input nodes, methods like Node2Vec will ignore it.</p>

  <h3 class="head1" id="sigil_toc_id_35">3.2.1 Node Similarity or Context</h3>

  <p class="body">We want our output embeddings to provide information about the relative positions of the nodes and their respective neighborhoods. One way to convey this type of information is via the concept of <b class="charbold">similarity</b>. In Euclidean domains, similarity often implies that entities have some geometric proximity, measured in terms of a distance and/or an angle. For a graph, proximity or distance can be interpreted as how many edges one must traverse (or hop) to get from one node to another. So for graphs, developed notions of similarity between two nodes can hinge upon how close these nodes are in terms of number of hops.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_03.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.3. Comparison of similarity concepts: (left) using distance on a plane, (right) using ‘hops’ along a graph.</p>
  </div>

  <p class="body">Another way to think about proximity is in terms of probability: given two nodes (node A and node B), what is the chance that I will encounter node B if I start to hop from node A? In the figure, if the number of hops is 1, the probability is zero, given there is no way to reach node B from node A in one hop. If the number of hops is 2, and we add the restriction that no node can be encountered twice in a traversal, and the assumption that each direction is equally likely, the probability is 20%.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_04.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.4. Illustrating the notion of proximity computed in terms of probability: given a walk from node A, the probability of encountering node B is a measure of proximity.</p>
  </div>

  <p class="body">To make that calculation, I visually inspected the figure and used counting. However, in the real world, calculating such probability-based proximity on large and complex graphs would become rather intractable.</p>

  <h3 class="head1" id="sigil_toc_id_36">3.2.2 Random Walks across Graphs</h3>

  <p class="body">Random walk approaches expand on the ideas above by using random walks across the graph. With these, similarity between two nodes A and B is defined as the probability that one will encounter node B on a random graph traversal from node A. These walks are unrestricted in comparison with our simple example above, in that there is no restriction that prevents a walk from backtracking or encountering the same node multiple times.</p>

  <p class="body">Deepwalk determines similarity in this way by enacting several random walks of a fixed size for each node, and calculating similarities from these. In these walks, any path is equally likely to occur, making them unbiased.</p>

  <p class="body">Node2Vec improved on this by introducing tunable bias in these random walks. The idea is to be able to trade off learnings from a node’s close-by neighborhood and from further away. Node2Vec captures this in two parameters:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;<i class="charitalics">p</i>: Tunes whether the path walked will return to the previous node.</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;<i class="charitalics">q</i>: Tunes whether a DFS (depth first search, a hopping strategy that emphasizes faraway nodes) and BFS (breadth first search, a strategy that emphasizes nearby nodes).</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;To mimic the DeepWalk algorithm, both <i class="charitalics">p</i> and <i class="charitalics">q</i> would be set to zero.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_05.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.5. Illustration of DFS and BFS on a graph.</p>
  </div>

  <h3 class="head1" id="sigil_toc_id_37">3.2.3 Optimization</h3>

  <p class="body">As with all learning problems, Node2Vec has:</p>

  <p class="listbullet">·&nbsp;&nbsp;&nbsp;An <i class="charitalics">objective function</i>. In this case, the log-probability of observing a node’s neighborhood:</p>

  <p class="pcalibre1"><code class="codea"><b class="codecharbold">log(Pr(Ns(u)|f(u))</b></code></p>

  <p class="listbody">This log-probability is conditioned on the node’s feature representation, f(u). This feature representation is the vector representation we want to end up with as an embedding.</p>

  <p class="listbullet">·&nbsp;&nbsp;&nbsp;An <i class="charitalics">optimization target.</i> In this case, we wish to maximize the above objective function. In the Node2Vec paper, this is handled using stochastic gradient ascent. In the form above, for large graphs, computing the optimization is costly. So, a few simplifications are made based on assumptions of conditional independence and symmetry in a node’s neighborhood.</p>

  <h3 class="head1" id="sigil_toc_id_38">3.2.4 Implementations and Uses of Node2Vec</h3>

  <p class="body">There are a few implementations of Node2Vec out there. For our demonstrations, we will use the Node2Vec library at <a class="pcalibre8" href="https://github.com/eliorc/node2vec">https://github.com/eliorc/node2vec</a>. As of this writing, it can be installed using:</p>

  <p class="pcalibre1"><code class="codea">pip&nbsp;install&nbsp;node2vec</code></p>

  <p class="body">Again, the aim here is to generate node embeddings from our social graph, then visualize them in 2D. For the visualization, we will use the T-SNE (pronounced tee snee) algorithm.</p>

  <div class="calibre15">
    <p class="sidebarhead">Inset: T-SNE</p>
  </div>

  <p class="sidebarbody">T-SNE, or T-distributed Stochastic Neighbor Embedding, is a dimensionality reduction technique. We will use it to reduce our node embedding vectors to two dimensions so that we can plot on a 2D figure.</p>

  <p class="sidebarbody">&nbsp;</p>

  <p class="sidebarbody">TSNE and Node2Vec have somewhat similar goals: to present data in a low dimensional vector. The differences are in the starting point, and in the algorithms and assumptions. T-SNE starts with a vector and produces a vector, in 2 or 3 dimensions. Node2Vec and other graph embedding techniques start with a graph, and produce a vector where the resulting dimension is much lower than the dimensionality of the graph.</p>

  <div class="calibre16">
    <p class="sidebarend">&nbsp;</p>
  </div>

  <p class="body">Let’s start by loading the Node2Vec library and our social graph data. We’ll generate node embeddings with 64 dimensions, then use T-SNE to further reduce these embeddings to 2 dimensions for plotting.</p>

  <p class="codealistingcaption">Listing 3.1. Read in our graph data, and import the Node2Vec library.</p>

  <p class="pcalibre1"><code class="codeacxspfirst">social_graph&nbsp;=&nbsp;nx.read_edgelist('edge_list2.txt')</code> <code class="codeacxspmiddle">social_graph.edges()</code> <code class="codeacxsplast">from&nbsp;node2vec&nbsp;import&nbsp;Node2Vec</code></p>

  <p class="codealistingcaption">Listing 3.2. Function to create edge list from relationship dictionary.</p>

  <p class="pcalibre1"><code class="codeacxspfirst">node2vec&nbsp;=&nbsp;Node2Vec(social_graph,&nbsp;dimensions=64,&nbsp;walk_length=30,&nbsp;num_walks=200,&nbsp;workers=4)&nbsp;&nbsp;#A</code> <code class="codeacxspmiddle">model&nbsp;=&nbsp;node2vec.fit(window=10,&nbsp;min_count=1,&nbsp;batch_words=4)&nbsp;&nbsp;#B&nbsp;Any&nbsp;keywords&nbsp;acceptable&nbsp;by&nbsp;gensim.Word2Vec&nbsp;can&nbsp;be&nbsp;passed,&nbsp;`dimensions`&nbsp;and&nbsp;`workers`&nbsp;are&nbsp;automatically&nbsp;passed&nbsp;(from&nbsp;the&nbsp;Node2Vec&nbsp;constructor)</code> <code class="codeacxsplast">model.wv.save_word2vec_format('EMBEDDING_FILENAME')&nbsp;#C</code></p>

  <p class="codeaannotation">#A&nbsp; Precompute probabilities and generate walks</p>

  <p class="codeaannotation">#B Generate embeddings. The batch_words parameter is because much of this function is derived from the Word2Vec algorithm.</p>

  <p class="codeaannotation">#C Save embedding in a file to disk.</p>

  <p class="body">With a file with the embeddings, we can read in the file by line, as with a text file. Then transform these lines into a list of lists.</p>

  <p class="codealistingcaption">Listing 3.3. Read in the file of embeddings. Transform the data into a list of lists, with each sub-list a separate node embedding.</p>

  <p class="pcalibre1"><code class="codeacxspfirst">with&nbsp;open("EMBEDDING_FILENAME",&nbsp;"r")&nbsp;as&nbsp;social:</code> <code class="codeacxspmiddle">&nbsp;&nbsp;&nbsp;&nbsp;lines&nbsp;=&nbsp;social.readlines()</code> <code class="codeacxspmiddle">embedded_lines&nbsp;=&nbsp;[x.split('&nbsp;')[1:]&nbsp;for&nbsp;x&nbsp;in&nbsp;lines[1:]]&nbsp;</code> <code class="codeacxspmiddle">n2v_embeddings&nbsp;=&nbsp;[]</code> <code class="codeacxspmiddle">for&nbsp;line&nbsp;in&nbsp;embedded_lines:</code> <code class="codeacxspmiddle">&nbsp;&nbsp;&nbsp;&nbsp;new_line&nbsp;=&nbsp;[float(y)&nbsp;for&nbsp;y&nbsp;in&nbsp;line]&nbsp;#B</code> <code class="codeacxsplast">&nbsp;&nbsp;&nbsp;&nbsp;n2v_embeddings.append(new_line)</code></p>

  <p class="codeaannotation">#A Transform the list of strings from the file read into a list of lists (of strings)</p>

  <p class="codeaannotation">#B Transform the list of lists of strings into a list of lists of floats.</p>

  <p class="body">Finally, we fit a T-SNE model using our embeddings. We select the first and second features and plot.</p>

  <p class="codealistingcaption">Listing 3.4. Function to create edge list from relationship dictionary.</p>

  <p class="pcalibre1"><code class="codeacxspfirst">tsne_model&nbsp;=&nbsp;TSNE(learning_rate=200)</code> <code class="codeacxspmiddle">tsne_features&nbsp;=&nbsp;tsne_model.fit_transform(n2v_embeddings)</code> <code class="codeacxspmiddle">&nbsp;</code> <code class="codeacxspmiddle">#&nbsp;Select&nbsp;the&nbsp;0th&nbsp;feature:&nbsp;xs</code> <code class="codeacxspmiddle">xs&nbsp;=&nbsp;tsne_features[:,0]</code> <code class="codeacxspmiddle">#&nbsp;Select&nbsp;the&nbsp;1st&nbsp;feature:&nbsp;ys</code> <code class="codeacxspmiddle">ys&nbsp;=&nbsp;tsne_features[:,1]</code> <code class="codeacxspmiddle">&nbsp;</code> <code class="codeacxspmiddle">plt.scatter(xs,ys)</code> <code class="codeacxsplast">plt.show()</code></p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_06.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.6. Embeddings of the social graph generated by Node2Vec. Colors correspond to <i class="charitalics">company_type</i>.</p>
  </div>

  <p class="body">We observe structures and distinct clusters in our 2D representation. Next, we’ll use a GNN with T-SNE to generate a similar plot.</p>

  <h2 class="head" id="sigil_toc_id_39">3.3 Inductive Embedding Technique: GNN</h2>

  <p class="body">In this section, we will briefly discuss a GNN used as an inductive method to generate node embeddings. We will then implement a simple GCN architecture using pytorch geometric. This GCN will generate the node embeddings for our social graph. Then in subsequent chapters, we’ll deep dive into the GCN, and other important GNN architectures.</p>

  <p class="body">Graph Neural Networks can be thought of as end-to-end machine learning on graphs. This is because previous methods of graph-based machine learning were piecemeal, combining the results of several separate and distinct processes. Before the advent of GNN methods, to perform node classification, one would use a process where a node embedding technique was used (like Node2Vec), and its output was rolled as a feature into a separate machine learning method (like a random forest or linear regression).</p>

  <p class="body">With GNNs, the representation learning is inherent in the architecture. So, in this section, we will take advantage of this and directly work with the learned embeddings produced from our social graph.</p>

  <p class="body">(Let us note that while the embedding described here with a GNN is an inductive method, transductive methods have been used with GNNs for downstream tasks [Rossi].)</p>

  <h3 class="head1" id="sigil_toc_id_40">3.3.1 Inductive Embeddings with a GNN</h3>

  <p class="body">In section 3.3.2, we outlined some of the characteristics and tradeoffs of transductive methods as applied to node embeddings. We list here some characteristics of inductive methods as a contrast:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;<b class="charbold">Node Features</b>: We learned that existing graph transductive methods don’t take into account node attributes. Inductive methods can and do incorporate attributes and node features when calculating embeddings.</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;<b class="charbold">Deep vs Shallow</b>: We noted that transductive node embedding methods are akin to a lookup table. The lookup table can be seen as the transformation layer that converts a node to its respective vector embedding.</p>

  <p class="listbody">For GNNs, our training algorithm results in fixing the parameters of a model.</p>

  <p class="listbullet">·&nbsp;&nbsp;&nbsp;<b class="charbold">Generalizable Model for Embeddings</b>: Our resulting model can be applied to unseen data, as opposed to the transductive way, which was limited to the training data.</p>

  <p class="body">Next, we take a high level look at GNNs and how we generate embeddings from them.</p>

  <h3 class="head1" id="sigil_toc_id_41">3.3.2 Deep Learning and GNNs</h3>

  <p class="body">Deep Learning methods in general are composed of building blocks, or layers, that take some tensor-based input, and produce an output that is transformed as it flows through the various layers. At the end, more transformations and aggregations are applied to yield a prediction. However, often the output of the hidden layers are directly exploited for other tasks within the model architecture or are used as inputs to other models.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_07.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.7. Layers in a multilayer perceptron.</p>
  </div>

  <p class="body">This same idea applies to graph neural networks. While the architecture of GNNs is starkly different in many ways from, say feed forward neural networks, there are some analogues. In many of the graph neural networks we will study, a graph in tensor form is passed into the GNN architecture, and one or more iterations of a process called <b class="charbold">message passing</b> is applied. These message passing processes could be thought of as layers, as shown in figure 3.8.</p>

  <p class="body">For a feed-forward network, like the one in figure 3.7, information is passed between the nodes of our neural network. In a GNN, message passing or information passing occurs across the vertices of the graph. Over each message passing step, each vertex collects information from vertices one hop away. So, if we want our node representations to take account of nodes from 3 hops away from each node, we conceivably need 3 message passing layers. There are diminishing returns from adding message passing layers, which we’ll explore in later chapters.</p>

  <p class="body">Different message passing schemes lead to different flavors of GNNs. So, for each GNN we study in this book, we’ll pay close attention to the math and code implementation for message passing.</p>

  <p class="body">After message passing, the resulting tensor is passed through feed-forward layers that result in a prediction. In the top of figure 3.8, which illustrates a GNN model for node prediction, after the data flows through message passing layers, the tensor then passes through an additional multi-layer perceptron and activation function to output a prediction.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_09a.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.8. (top) A simple GNN architecture diagram. A graph is input on the left, encountering node-information-passing layers. This is followed by multi-layer perceptron layers. After an activation is applied, a prediction is yielded. (bottom) A graph neural network architecture, similar to the upper figure, but after passing through the message passing layers, instead of passing through MLP layers and an activation, the output data can be used as embeddings.</p>
  </div>

  <p class="body">However, as with the feed-forward neural network illustrated above, we can extract the output of any of the hidden layers and work directly with that output. For GNNs, the output of hidden message passing layers are node embeddings.</p>

  <h3 class="head1" id="sigil_toc_id_42">3.3.3 Using Pytorch Geometric</h3>

  <p class="body">Pytorch Geometric, like Pytorch, has several built in GNN layers, including for graph convolutional networks, the type we will use in our example. GCNs are often used for node classification tasks, which we will cover in chapter 4. For now, we will just extract the embedding from our last GCN layer, visualize them using T-SNE. From the first half of this chapter, we know how to create a pytorch dataset object, and will use that here.</p>

  <h3 class="head1" id="sigil_toc_id_43">3.3.4 Our Process/Pipeline</h3>

  <p class="body">To generate node embeddings from our social graph, we:</p>

  <p class="listnumberedcxspfirst">1.&nbsp;&nbsp;Begin with its adjacency list file,</p>

  <p class="listnumberedcxsplast">2.&nbsp;&nbsp;Load it into a Pytorch Geometric dataset.</p>

  <p class="listnumberedcxsplast">3.&nbsp;&nbsp;Initialize a GNN model.</p>

  <p class="listnumberedcxsplast">4.&nbsp;&nbsp;Pass our data through this GNN to generate embeddings</p>

  <p class="body">For preprocessing, we will load the data into a pytorch geometric <i class="charitalics">dataset</i> object using the script we already created in section 3.2.</p>

  <p class="body">We will then create a simple architecture with three GCN layers (from PyG’s built-in layers) to produce our embeddings.</p>

  <p class="body">Unlike in later chapters, there is no need to train a model to make the embeddings. We could refine our embeddings against some criteria by doing training, which is exactly how GNNs work: the graph embeddings as well as the neural network parameters are optimized for prediction tasks.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_09.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.9. Flowchart of this section’s process.</p>
  </div>

  <p class="body">Upon creating the dataset, we can inspect the graph’s and dataset’s properties.</p>

  <p class="codealistingcaption">Listing 3.5. Commands to create dataset using code in listing 3.8, and to gather summary statistics about the graph.</p>

  <p class="pcalibre1"><code class="codeacxspfirst">dataset&nbsp;=&nbsp;MyOwnDataset('')</code> <code class="codeacxspmiddle">data&nbsp;=&nbsp;dataset[0]</code> <code class="codeacxspmiddle">print(f'Number&nbsp;of&nbsp;nodes:&nbsp;{data.num_nodes}')</code> <code class="codeacxspmiddle">print(f'Number&nbsp;of&nbsp;edges:&nbsp;{data.num_edges}')</code> <code class="codeacxspmiddle">print(f'Average&nbsp;node&nbsp;degree:&nbsp;{data.num_edges&nbsp;/&nbsp;data.num_nodes:.2f}')</code> <code class="codeacxspmiddle">print(f'Contains&nbsp;isolated&nbsp;nodes:&nbsp;{data.contains_isolated_nodes()}')</code> <code class="codeacxspmiddle">print(f'Contains&nbsp;self-loops:&nbsp;{data.contains_self_loops()}')</code> <code class="codeacxspmiddle">print(f'Is&nbsp;undirected:&nbsp;{data.is_undirected()}')</code> <code class="codeacxspmiddle">Data(edge_index=[2,&nbsp;12628],&nbsp;x=[1947,&nbsp;1],&nbsp;y=[1947])</code> <code class="codeacxspmiddle">===========================================================================================================</code> <code class="codeacxspmiddle">Number&nbsp;of&nbsp;nodes:&nbsp;1947</code> <code class="codeacxspmiddle">Number&nbsp;of&nbsp;edges:&nbsp;12628</code> <code class="codeacxspmiddle">Average&nbsp;node&nbsp;degree:&nbsp;6.49</code> <code class="codeacxspmiddle">Contains&nbsp;isolated&nbsp;nodes:&nbsp;False</code> <code class="codeacxspmiddle">Contains&nbsp;self-loops:&nbsp;False</code> <code class="codeacxsplast">Is&nbsp;undirected:&nbsp;False</code></p>

  <h3 class="head1" id="sigil_toc_id_44">3.3.5 The GNN architecture for our example</h3>

  <p class="body">Let’s take a closer look at the layers in our example architecture.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_10.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.10.&nbsp; Architecture for this example. The left-hand simbol is the input graph.</p>
  </div>

  <p class="body">The GNN layers consist of:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;A <span class="charunderline">Message Passing layer</span>:&nbsp; where information gets passed between vertices</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;An <span class="charunderline">Activation layer</span>: where a non-linear function is applied to the previous output</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;A <span class="charunderline">Dropout layer</span>: switching off some of the output units.</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;A <span class="charunderline">Layer Normalization layer</span>: a normalization of the activated output to a mean of zero and a variance of 1.</p>

  <p class="body">The feed-forward layers consist of:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;A <span class="charunderline">Linear</span> layer: A classic linear function, in the form of a single line of input neural nodes.</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;A Dropout Layer</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;A second Linear Layer</p>

  <p class="body">In pytorch, a way to create neural network architectures is by creating a class which inherits from the <i class="charitalics">torch.nn.Module</i> class. Listing 3.6 shows the our class, consisting of:</p>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;An <i class="charitalics">__init__</i> method, which defines and initializes parameter weights for our layers.</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;A <i class="charitalics">forward</i> method, which governs how data passes forward through our architecture.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_11.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.11 - (top) Diagram of GNN layers: Message Passing layer, Activation, Dropout, and Layer Normalization. (bottom) Diagram of our Feed Forward layers:&nbsp; Linear Layer, Dropout, and a second Linear Layer.</p>
  </div>

  <p class="body">In addition, there are a few hyper-parameters related to the input and output sizes of the message passing, layer norm, and linear layers, and the dropout.</p>

  <p class="figurea"><img alt="" class="calibre21" src="../Images/Listing_3.6.png" /><br class="calibre22" /></p>

  <p class="body">We’ve created our dataset and our GNN architecture. In listing 3.7, we pass the graph data through our architecture and visualize the resulting embeddings. There is no model training involved, only a pass through, which consists of two lines of code. The first line initializes our model and its weights. The second line passes our graph data through this model.</p>

  <p class="codealistingcaption">Listing 3.7. Code to pass graph data through the GNN and plot the results.</p>

  <p class="pcalibre1"><code class="codeacxspfirst">model&nbsp;=&nbsp;SimpleGNN()&nbsp;#A</code> <code class="codeacxspmiddle">&nbsp;</code> <code class="codeacxspmiddle">embedding,&nbsp;out&nbsp;=&nbsp;model(data)&nbsp;#B</code> <code class="codeacxspmiddle">&nbsp;</code> <code class="codeacxspmiddle">color_list&nbsp;=&nbsp;["red",&nbsp;"orange",&nbsp;"green",&nbsp;"blue",&nbsp;"purple",&nbsp;"brown",&nbsp;"pink",&nbsp;"gray",&nbsp;"olive",&nbsp;"cyan",&nbsp;"lime"]&nbsp;#C1</code> <code class="codeacxspmiddle">colors&nbsp;=&nbsp;[]&nbsp;#C2</code> <code class="codeacxspmiddle">colors&nbsp;+=&nbsp;[color_list[y]&nbsp;for&nbsp;y&nbsp;in&nbsp;data.y.detach().numpy()]&nbsp;#C3</code> <code class="codeacxspmiddle">&nbsp;</code> <code class="codeacxspmiddle">xs,&nbsp;ys&nbsp;=&nbsp;zip(*TSNE().fit_transform(embedding.detach().numpy()))&nbsp;#D</code> <code class="codeacxsplast">plt.scatter(xs,&nbsp;ys,&nbsp;c=colors)&nbsp;#E</code></p>

  <p class="codeaannotation">#A Initializes our GNN model and its weights</p>

  <p class="codeaannotation">#B Passes graph data through our GNN architecture.</p>

  <p class="codeaannotation">#C An optional step to colorize the output by one of our attributes, company_type:</p>

  <p class="codeaannotation">#D1 A list of colors at least as long as the unique company types in our data</p>

  <p class="codeaannotation">#D2 initialize a list for the color parameter used in our matplotlib <i class="charitalics">scatter</i> method</p>

  <p class="codeaannotation">#D3 For each node, the company_type feature is encoded simply as an integer. These integers correspond to the colors in the <i class="charitalics">color_list.</i> For every vertex in our graph, we have a color to match its company_type.</p>

  <p class="codeaannotation">#D Using T-SNE to reduce the dimensionality of our output embeddings.</p>

  <p class="codeaannotation">#E Creating a scatter-plot of the embeddings in 2-D.</p>

  <p class="body">Finally, we have the visualization of the node embeddings. We see clusters, but no correlation between company type and the clusters. By adjusting the GNN layers, the hyper-parameters, or the graph data (we have given every node a unit feature in this example), we can adjust the embeddings output. Ultimately, for a given application, the performance of the application will determine the efficacy of the embeddings.</p>

  <div class="figure">
    <p class="figurea"><img alt="" class="calibre21" src="../Images/03_12.png" /><br class="calibre22" /></p>

    <p class="figureacaption">Figure 3.12. Visualization of embeddings generated from GNN. Colors correspond to <i class="charitalics">company_type</i>.</p>
  </div>

  <h2 class="head" id="sigil_toc_id_45">3.4 Summary</h2>

  <p class="listbulletcxspfirst">·&nbsp;&nbsp;&nbsp;Node and Graph embeddings are powerful methods to extract insights from our data, and can serve as inputs/features in our machine learning models. There are several independent methods for generating such embeddings. Graph Neural Networks have embedding built into the architecture.</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;For producing embeddings methods can be classed as inductive or transductive. Inductive learning methods are akin to supervised learning. GNNs are inductive methods. Transductive methods learn and ‘predicts’ on a closed dataset, train on labeled and unlabeled data, and basically acts as a lookup table rather than a predictive model. Random walk methods like Node2Vec are transductive.</p>

  <p class="listbulletcxspmiddle">·&nbsp;&nbsp;&nbsp;Node Embeddings can directly be used as features in traditional machine learning models, whether from transductive methods or from the output of a GNN.</p>

  <p class="listbulletcxsplast">·&nbsp;&nbsp;&nbsp;Embeddings are low-dimensional representations of graph objects. By using dimensionality reduction techniques like T-SNE to further reduce embeddings to&nbsp; two dimensions, we can visualize our graphs, and draw further insights by inspection.</p>

  <h2 class="head" id="sigil_toc_id_46">3.5 References</h2>

  <p class="body">Grover, Aditya, and Jure Leskovec. "node2vec: Scalable feature learning for networks." <i class="charitalics">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</i>. 2016.</p>

  <p class="body">Duong, Chi Thang, et al. "On node features for graph neural networks." arXiv preprint arXiv:1911.08795 (2019).</p>

  <p class="body">Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. "Deepwalk: Online learning of social representations." Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014.</p>

  <p class="body">Hamilton, William L. "Graph representation learning." Synthesis Lectures on Artificial Intelligence and Machine Learning 14.3 (2020): 1-159.</p>

  <p class="body">Rossi, A., Tiezzi, M., Dimitri, G.M., Bianchini, M., Maggini, M., &amp; Scarselli, F. (2018). "Inductive–Transductive Learning with Graph Neural Networks", In Artificial Neural Networks in Pattern Recognition (pp.201-212). Berlin : Springer-Verlag.</p>
</body>
</html>
